[
  {
    "id": "csv_storage",
    "term": "CSV 存储",
    "category": "数据处理与去重",
    "core": "CSV 存储轻量、通用，适合中小规模数据交换与初期落地。",
    "boundary": "把 CSV 当高并发数据库使用会出现读写冲突和性能瓶颈。",
    "signal": "原型阶段或跨工具导入导出需求明显时，CSV 是低门槛选择。",
    "action": "固定列顺序和编码（UTF-8）；写入前做字段清洗并加表头版本注释。",
    "aliases": [
      "csv"
    ]
  },
  {
    "id": "sqlite_storage",
    "term": "SQLite 存储",
    "category": "数据处理与去重",
    "core": "SQLite 提供单文件事务数据库能力，适合本地任务和轻量服务的数据持久化。",
    "boundary": "在高并发写场景强用 SQLite 会遇到锁竞争。",
    "signal": "需要比 CSV 更可靠的查询和约束，但又不想引入独立数据库服务时可选 SQLite。",
    "action": "为关键字段建索引和唯一约束；定期 `VACUUM` 并备份数据库文件。",
    "aliases": [
      "sqlite"
    ]
  },
  {
    "id": "unique_index",
    "term": "唯一索引",
    "category": "数据处理与去重",
    "core": "唯一索引通过数据库约束从源头防重，优于应用层“先查后写”的脆弱方案。",
    "boundary": "只在代码里判断重复，在并发下仍会写入脏数据。",
    "signal": "同一记录重复入库或重复通知时，应优先加唯一索引。",
    "action": "根据业务主键（如 source_id+date）建立 UNIQUE；冲突时采用 upsert 策略。",
    "aliases": [
      "unique",
      "索引"
    ]
  },
  {
    "id": "url_hash",
    "term": "URL 哈希",
    "category": "数据处理与去重",
    "core": "URL 哈希把长链接映射为固定长度键，便于去重和快速比较。",
    "boundary": "不做 URL 规范化就哈希，会把同一资源误判为不同记录。",
    "signal": "采集系统需要大量链接去重时，URL 哈希是常见基建手段。",
    "action": "先做规范化（去追踪参数、统一协议），再计算 hash 并持久化原始 URL 供追溯。",
    "aliases": [
      "hash",
      "url"
    ]
  },
  {
    "id": "timestamping",
    "term": "时间戳标准化",
    "category": "数据处理与去重",
    "core": "时间戳标准化确保跨系统数据可比较、可排序、可追踪。",
    "boundary": "混用本地时间和 UTC 会导致数据对齐混乱。",
    "signal": "多数据源合并或跨时区分析时，时间字段处理必须统一。",
    "action": "入库统一存 UTC ISO8601；展示层再按用户时区转换。",
    "aliases": [
      "timestamp",
      "UTC"
    ]
  },
  {
    "id": "data_cleaning",
    "term": "数据清洗",
    "category": "数据处理与去重",
    "core": "数据清洗用于处理缺失、异常、重复和格式不一致，决定下游分析可信度。",
    "boundary": "跳过清洗直接分析，得到的结论可能方向性错误。",
    "signal": "字段缺失率上升、统计结果异常波动时，优先检查清洗规则。",
    "action": "定义清洗流水线：去空值、标准化、异常值处理、去重；每步记录样本变化。",
    "aliases": [
      "清洗",
      "normalize"
    ]
  },
  {
    "id": "data_schema",
    "term": "数据字段规范（Schema）",
    "category": "数据处理与去重",
    "core": "Schema 规定字段名、类型和约束，是跨模块数据协作的共同语言。",
    "boundary": "字段随意增删改会让解析器和报表频繁崩溃。",
    "signal": "当数据需要跨团队或跨服务传递时，schema 先于功能开发。",
    "action": "用 JSON Schema 或表结构文档固化约束；变更走版本化和兼容窗口。",
    "aliases": [
      "schema",
      "字段规范"
    ]
  },
  {
    "id": "raw_snapshot",
    "term": "原始快照回放",
    "category": "数据处理与去重",
    "core": "原始快照回放保留“未经处理”的输入证据，便于复盘和规则回归测试。",
    "boundary": "只保存处理后结果，一旦规则有误无法还原现场。",
    "signal": "采集规则频繁迭代或数据争议较多时，快照回放价值极高。",
    "action": "按批次存原始 payload 并附采集时间；修规则后先对历史快照回放验证。",
    "aliases": [
      "回放",
      "snapshot"
    ]
  },
  {
    "id": "incremental_update",
    "term": "增量更新",
    "category": "数据处理与去重",
    "core": "增量更新只处理变化部分，能显著降低任务耗时和资源消耗。",
    "boundary": "每次全量重跑不仅慢，还容易触发重复写入和重复通知。",
    "signal": "数据规模增长导致窗口内跑不完时，应切换增量策略。",
    "action": "维护游标（时间戳/ID）；每次只拉取游标之后数据，成功后再推进游标。",
    "aliases": [
      "增量",
      "cursor"
    ]
  },
  {
    "id": "data_quality_check",
    "term": "数据质量校验",
    "category": "数据处理与去重",
    "core": "数据质量校验通过规则和指标持续监控数据健康，防止坏数据静默扩散。",
    "boundary": "只在入库时报错而无持续监控，异常会在下游才被发现。",
    "signal": "报表突变或模型效果下降时，往往先是数据质量出问题。",
    "action": "设置完整性、唯一性、范围合法性三类检查；异常时阻断下游并告警。",
    "aliases": [
      "质量",
      "校验"
    ]
  }
]
